{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"migrants_slovenian_to_english_labbert.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1AFV0ArRkbFlwCImXSsE31753qbpi0IvJ","authorship_tag":"ABX9TyO6bBDmkihfZ9GX5XU7vQR9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"Y0lvAoF75rfT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":742},"outputId":"4b6dda0e-f45d-4baa-82be-7a47935de648","executionInfo":{"status":"ok","timestamp":1587574819744,"user_tz":-120,"elapsed":9799,"user":{"displayName":"Žan Pečovnik","photoUrl":"","userId":"03601017400239052827"}}},"source":["pip install transformers pytorch-pretrained-bert"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n","\u001b[K     |████████████████████████████████| 573kB 6.5MB/s \n","\u001b[?25hCollecting pytorch-pretrained-bert\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n","\u001b[K     |████████████████████████████████| 133kB 38.4MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/50/93509f906a40bffd7d175f97fd75ea328ad9bd91f48f59c4bd084c94a25e/sacremoses-0.0.41.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 38.8MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Collecting tokenizers==0.5.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n","\u001b[K     |████████████████████████████████| 3.7MB 35.5MB/s \n","\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.40)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n","\u001b[K     |████████████████████████████████| 1.0MB 46.2MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n","Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.4.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n","Requirement already satisfied: botocore<1.16.0,>=1.15.40 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.40)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.40->boto3->transformers) (2.8.1)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.40->boto3->transformers) (0.15.2)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.41-cp36-none-any.whl size=893334 sha256=0292d56b1f677bbc96d86da7263fc255b87af372b3bf9a2aeb48dc54f2ee402a\n","  Stored in directory: /root/.cache/pip/wheels/22/5a/d4/b020a81249de7dc63758a34222feaa668dbe8ebfe9170cc9b1\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers, pytorch-pretrained-bert\n","Successfully installed pytorch-pretrained-bert-0.6.2 sacremoses-0.0.41 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.8.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bM8F3Wz4Iqh6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"outputId":"0f0ce086-96c9-4177-811e-d94c88779dcd","executionInfo":{"status":"ok","timestamp":1587574823018,"user_tz":-120,"elapsed":12923,"user":{"displayName":"Žan Pečovnik","photoUrl":"","userId":"03601017400239052827"}}},"source":["import torch \n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":2,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla K80\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XhoxFPL7IqQW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"outputId":"c30950be-8588-4c2d-b011-66c550d5a7b7","executionInfo":{"status":"ok","timestamp":1587574854161,"user_tz":-120,"elapsed":43918,"user":{"displayName":"Žan Pečovnik","photoUrl":"","userId":"03601017400239052827"}}},"source":["from transformers import BertForSequenceClassification, AdamW\n","from pytorch_pretrained_bert import BertTokenizer\n","\n","# Reload the saved model and vocabulary\n","print('Reloading the saved model and vocabulary...')\n","\n","TRAINED_LGBT_BERT = './drive/My Drive/Colab Notebooks/Bert models/LabBert_migrants_slovenian/'\n","\n","model = BertForSequenceClassification.from_pretrained(TRAINED_LGBT_BERT)\n","tokenizer = BertTokenizer.from_pretrained(TRAINED_LGBT_BERT, do_lower_case=False)\n","\n","# Tell the model to run on the GPU\n","model.cuda()\n","\n","optimizer = AdamW(model.parameters(), \n","                lr=2e-5, # learning rate, default = 5e-5\n","                eps=1e-8 # adam_epsilon, default = 1e-8\n","                )\n","\n","print('Done...')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Reloading the saved model and vocabulary...\n","Done...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5KqFDH_PIqNB","colab_type":"code","colab":{}},"source":["import numpy as np\n","import time\n","import datetime\n","\n","# Helper function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","  pred_flat = np.argmax(preds, axis=1).flatten()\n","  labels_flat = labels.flatten()\n","  return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","# Helper function for formatting elapsed times\n","def format_time(elapsed):\n","  '''\n","  Takes a time in seconds and returns a string in format hh:mm:ss\n","  '''\n","  elapsed_rounded = int(round(elapsed))\n","  return str(datetime.timedelta(seconds=elapsed_rounded))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lcR1FcS3IqKu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":84},"outputId":"5978a059-fbf8-45b9-9ed9-e7ced123c890","executionInfo":{"status":"ok","timestamp":1587574862046,"user_tz":-120,"elapsed":51493,"user":{"displayName":"Žan Pečovnik","photoUrl":"","userId":"03601017400239052827"}}},"source":["import pandas as pd\n","import gc\n","import math\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from torch.optim.lr_scheduler import StepLR\n","\n","MAX_LENGTH = 499\n","BATCH_SIZE = 8\n","\n","# Load the english data\n","print('Loading the english data...')\n","\n","CSV_FILE_PATH = './drive/My Drive/Colab Notebooks/diploma_data/migrants_islamophobia_final.csv'\n","df_slo = pd.read_csv(CSV_FILE_PATH, sep=',', header=None, names=['comment', 'label'])\n","\n","comments = df_slo.comment.values\n","labels = df_slo.label.values\n","\n","# Tokenize all of the comments and map the tokens to their word IDs\n","input_ids=[]\n","input_labels=[]\n","\n","for i,comment in enumerate(comments):\n","\n","  if isinstance(comment, float):\n","    continue\n","\n","  tokenized_comment = tokenizer.tokenize(comment)\n","  tokenized_comment.insert(0, '[CLS]')\n","  tokenized_comment.append('[SEP]')\n","\n","  if len(tokenized_comment) > 512:\n","    continue\n","  \n","  comment_ids = tokenizer.convert_tokens_to_ids(tokenized_comment)\n","  input_ids.append(comment_ids)\n","  input_labels.append(labels[i])\n","\n","# Delete the comments and labels so I free memory\n","del comments\n","del labels\n","gc.collect()\n","\n","print('Size of english dataset: {:,}'.format(len(input_ids)))\n","\n","print('Padding/truncating all the sentences to %d values...' % MAX_LENGTH)\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LENGTH, dtype='long', value=0, truncating='post', padding='post')\n","\n","attention_masks= []\n","for cmnt in input_ids:\n","  att_mask = [int(token_id > 0) for token_id in cmnt]\n","  attention_masks.append(att_mask)\n","\n","# size of training set is 80%, size of validation set is 20%\n","# random_state makes sure that the splitting is always the same\n","# split the validation set to actual validation set and testing set which is 10% of original size\n","train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, input_labels, random_state=420, test_size=0.2) \n","validation_inputs, test_inputs, validation_labels, test_labels = train_test_split(validation_inputs, validation_labels, random_state=420, test_size=0.5) # 0.5 * 0.2 = 0.1 -> 10% size of testing set\n","\n","train_masks, validation_masks, _, validation_masks_labels = train_test_split(attention_masks, input_labels, random_state=420, test_size=0.2)\n","validation_masks, test_masks, _, _ = train_test_split(validation_masks, validation_masks_labels, random_state=420, test_size=0.5)\n","\n","train_data = TensorDataset(torch.tensor(train_inputs), torch.tensor(train_masks), torch.tensor(train_labels))\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n","\n","validation_data = TensorDataset(torch.tensor(validation_inputs), torch.tensor(validation_masks), torch.tensor(validation_labels))\n","validation_sampler = SequentialSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=BATCH_SIZE)\n","\n","test_data = TensorDataset(torch.tensor(test_inputs), torch.tensor(test_masks), torch.tensor(test_labels))\n","test_sampler = SequentialSampler(test_data)\n","test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)\n","\n","# Create the learning rate scheduler\n","scheduler = StepLR(optimizer, step_size=5, gamma=0.1)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["Loading the english data...\n","Size of english dataset: 5,821\n","Padding/truncating all the sentences to 499 values...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ltbQpssHIqIY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":101},"outputId":"8b459655-19e6-4114-ab2b-a2c55b1f94ee","executionInfo":{"status":"ok","timestamp":1587574900712,"user_tz":-120,"elapsed":89945,"user":{"displayName":"Žan Pečovnik","photoUrl":"","userId":"03601017400239052827"}}},"source":["###################\n","#     Testing     #\n","###################\n","# Measure the performance on the english testing set.\n","\n","testing_accuracy_values = []\n","\n","print('Testing on english data without additional training...')\n","\n","start_time = time.time()\n","\n","# Put the model in evaluation mode - the dropout layers behave differently during evaluation.\n","model.eval()\n","\n","# Tracking variables\n","test_accuracy = 0\n","num_of_batches = 0\n","\n","print('Number of testing comments: {:,}'.format(len(test_inputs)))\n","# Evaluate data for one epoch.\n","for batch in test_dataloader:\n","\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","\n","  # Unpack the inputs from out dataloader\n","  batch_input_ids, batch_attention_mask, batch_labels = batch\n","\n","  # Telling the model not to compute or store gradients, saves memory and speeds up validation\n","  with torch.no_grad():\n","      # Forward pass\n","      outputs = model(batch_input_ids, token_type_ids=None,\n","                      attention_mask=batch_attention_mask)\n","  \n","  # Get the \"logits\" output by the model, \"logits\" are the output values prior to applying an activation function\n","  logits = outputs[0]\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = batch_labels.to('cpu').numpy()\n","\n","  # Calculate the accuracy for this batch of test sentences\n","  acc = flat_accuracy(logits, label_ids)\n","\n","  # Accumulate the total accuracy\n","  test_accuracy += acc\n","\n","  # Track the number of batches\n","  num_of_batches += 1\n","\n","# Report the final accuracy for this testing run\n","accuracy = test_accuracy / num_of_batches\n","testing_accuracy_values.append(accuracy)\n","print('Accuracy: {0:.3f}'.format(accuracy))\n","print('Testing took: {:}'.format(format_time(time.time() - start_time)))\n","print('Testing done...')"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Testing on english data without additional training...\n","Number of testing comments: 583\n","Accuracy: 0.534\n","Testing took: 0:00:39\n","Testing done...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3pnnez2s1Sg1","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}