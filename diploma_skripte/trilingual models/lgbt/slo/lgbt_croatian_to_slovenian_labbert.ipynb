{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lgbt_croatian_to_slovenian_labbert.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1UYNryV5hoOOiFxD2g7wqmtSFcTPZcahY","authorship_tag":"ABX9TyMSLth0E91GQ5qcTtRFVeat"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"InHh4oiS4evq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":742},"outputId":"a55373a5-05fc-4bcf-977f-3f1fc3f0238c","executionInfo":{"status":"ok","timestamp":1587574030396,"user_tz":-120,"elapsed":8879,"user":{"displayName":"Žan Pečovnik","photoUrl":"","userId":"03601017400239052827"}}},"source":["pip install transformers pytorch-pretrained-bert"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n","\r\u001b[K     |▋                               | 10kB 27.5MB/s eta 0:00:01\r\u001b[K     |█▏                              | 20kB 5.8MB/s eta 0:00:01\r\u001b[K     |█▊                              | 30kB 8.2MB/s eta 0:00:01\r\u001b[K     |██▎                             | 40kB 10.5MB/s eta 0:00:01\r\u001b[K     |███                             | 51kB 6.9MB/s eta 0:00:01\r\u001b[K     |███▌                            | 61kB 8.0MB/s eta 0:00:01\r\u001b[K     |████                            | 71kB 9.1MB/s eta 0:00:01\r\u001b[K     |████▋                           | 81kB 10.2MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 92kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 102kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 112kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████                         | 122kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 133kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 143kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 153kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 163kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 174kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 184kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 194kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 204kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 215kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 225kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 235kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 245kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 256kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 266kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 276kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 286kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 296kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 307kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 317kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 327kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 337kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 348kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 358kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 368kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 378kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 389kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 399kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 409kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 419kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 430kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 440kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 450kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 460kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 471kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 481kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 491kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 501kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 512kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 522kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 532kB 9.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 542kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 552kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 563kB 9.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 573kB 9.0MB/s \n","\u001b[?25hCollecting pytorch-pretrained-bert\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n","\u001b[K     |████████████████████████████████| 133kB 60.0MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n","Collecting tokenizers==0.5.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n","\u001b[K     |████████████████████████████████| 3.7MB 48.7MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.40)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n","\u001b[K     |████████████████████████████████| 1.0MB 54.4MB/s \n","\u001b[?25hCollecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/50/93509f906a40bffd7d175f97fd75ea328ad9bd91f48f59c4bd084c94a25e/sacremoses-0.0.41.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 46.0MB/s \n","\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.4.0)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n","Requirement already satisfied: botocore<1.16.0,>=1.15.40 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.40)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.40->boto3->transformers) (0.15.2)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.40->boto3->transformers) (2.8.1)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.41-cp36-none-any.whl size=893334 sha256=30fbd56d25dc375510a70ed8ad38fcc69902dd79f4273b6acb0ff7e37d3cab7d\n","  Stored in directory: /root/.cache/pip/wheels/22/5a/d4/b020a81249de7dc63758a34222feaa668dbe8ebfe9170cc9b1\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers, pytorch-pretrained-bert\n","Successfully installed pytorch-pretrained-bert-0.6.2 sacremoses-0.0.41 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.8.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WVCR1FvTEEgY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"outputId":"77846084-49ad-428d-ce2a-0e871708c41b","executionInfo":{"status":"ok","timestamp":1587574033353,"user_tz":-120,"elapsed":11532,"user":{"displayName":"Žan Pečovnik","photoUrl":"","userId":"03601017400239052827"}}},"source":["import torch \n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":2,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla P4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UsLzaFY6EEdA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"outputId":"326e2e5a-0786-44dd-b65a-b149bf041961","executionInfo":{"status":"ok","timestamp":1587574068891,"user_tz":-120,"elapsed":46887,"user":{"displayName":"Žan Pečovnik","photoUrl":"","userId":"03601017400239052827"}}},"source":["from transformers import BertForSequenceClassification, AdamW\n","from pytorch_pretrained_bert import BertTokenizer\n","\n","# Reload the saved model and vocabulary\n","print('Reloading the saved model and vocabulary...')\n","\n","TRAINED_LGBT_BERT = './drive/My Drive/Colab Notebooks/Bert models/LabBert_lgbt_croatian/'\n","\n","model = BertForSequenceClassification.from_pretrained(TRAINED_LGBT_BERT)\n","tokenizer = BertTokenizer.from_pretrained(TRAINED_LGBT_BERT, do_lower_case=False)\n","\n","# Tell the model to run on the GPU\n","model.cuda()\n","\n","optimizer = AdamW(model.parameters(), \n","                lr=2e-5, # learning rate, default = 5e-5\n","                eps=1e-8 # adam_epsilon, default = 1e-8\n","                )\n","\n","print('Done...')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Reloading the saved model and vocabulary...\n","Done...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"76r7Ss1hEEZa","colab_type":"code","colab":{}},"source":["import numpy as np\n","import time\n","import datetime\n","\n","# Helper function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","  pred_flat = np.argmax(preds, axis=1).flatten()\n","  labels_flat = labels.flatten()\n","  return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","# Helper function for formatting elapsed times\n","def format_time(elapsed):\n","  '''\n","  Takes a time in seconds and returns a string in format hh:mm:ss\n","  '''\n","  elapsed_rounded = int(round(elapsed))\n","  return str(datetime.timedelta(seconds=elapsed_rounded))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fyISJu7SEEWC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":84},"outputId":"7c5396b5-08d5-4f95-f01e-acfcd34a16ae","executionInfo":{"status":"ok","timestamp":1587574074158,"user_tz":-120,"elapsed":51859,"user":{"displayName":"Žan Pečovnik","photoUrl":"","userId":"03601017400239052827"}}},"source":["import pandas as pd\n","import gc\n","import math\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from torch.optim.lr_scheduler import StepLR\n","\n","MAX_LENGTH = 499\n","BATCH_SIZE = 8\n","\n","# Load the slovenian data\n","print('Loading the slovenian data...')\n","\n","CSV_FILE_PATH_SLO = './drive/My Drive/Colab Notebooks/diploma_data/lgbt_homofobija_final.csv'\n","df_slo = pd.read_csv(CSV_FILE_PATH_SLO, sep=',', header=None, names=['comment', 'label'])\n","\n","comments = df_slo.comment.values\n","labels = df_slo.label.values\n","\n","# Tokenize all of the comments and map the tokens to their word IDs\n","input_ids=[]\n","input_labels=[]\n","\n","for i,comment in enumerate(comments):\n","\n","  if isinstance(comment, float):\n","    continue\n","\n","  tokenized_comment = tokenizer.tokenize(comment)\n","  tokenized_comment.insert(0, '[CLS]')\n","  tokenized_comment.append('[SEP]')\n","\n","  if len(tokenized_comment) > 512:\n","    continue\n","  \n","  comment_ids = tokenizer.convert_tokens_to_ids(tokenized_comment)\n","  input_ids.append(comment_ids)\n","  input_labels.append(labels[i])\n","\n","# Delete the comments and labels so I free memory\n","del comments\n","del labels\n","gc.collect()\n","\n","print('Size of slovene dataset: {:,}'.format(len(input_ids)))\n","\n","print('Padding/truncating all the sentences to %d values...' % MAX_LENGTH)\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LENGTH, dtype='long', value=0, truncating='post', padding='post')\n","\n","attention_masks= []\n","for cmnt in input_ids:\n","  att_mask = [int(token_id > 0) for token_id in cmnt]\n","  attention_masks.append(att_mask)\n","\n","# size of training set is 80%, size of validation set is 20%\n","# random_state makes sure that the splitting is always the same\n","# split the validation set to actual validation set and testing set which is 10% of original size\n","train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, input_labels, random_state=420, test_size=0.2) \n","validation_inputs, test_inputs, validation_labels, test_labels = train_test_split(validation_inputs, validation_labels, random_state=420, test_size=0.5) # 0.5 * 0.2 = 0.1 -> 10% size of testing set\n","\n","train_masks, validation_masks, _, validation_masks_labels = train_test_split(attention_masks, input_labels, random_state=420, test_size=0.2)\n","validation_masks, test_masks, _, _ = train_test_split(validation_masks, validation_masks_labels, random_state=420, test_size=0.5)\n","\n","train_data = TensorDataset(torch.tensor(train_inputs), torch.tensor(train_masks), torch.tensor(train_labels))\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n","\n","validation_data = TensorDataset(torch.tensor(validation_inputs), torch.tensor(validation_masks), torch.tensor(validation_labels))\n","validation_sampler = SequentialSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=BATCH_SIZE)\n","\n","test_data = TensorDataset(torch.tensor(test_inputs), torch.tensor(test_masks), torch.tensor(test_labels))\n","test_sampler = SequentialSampler(test_data)\n","test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)\n","\n","# Create the learning rate scheduler\n","scheduler = StepLR(optimizer, step_size=5, gamma=0.1)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Loading the slovenian data...\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["Size of slovene dataset: 4,460\n","Padding/truncating all the sentences to 499 values...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"w-516m_hEESj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":101},"outputId":"858cbf95-7eb2-45a2-894f-f3e245a1d244","executionInfo":{"status":"ok","timestamp":1587574090265,"user_tz":-120,"elapsed":67746,"user":{"displayName":"Žan Pečovnik","photoUrl":"","userId":"03601017400239052827"}}},"source":["###################\n","#     Testing     #\n","###################\n","# Measure the performance on the slovenian testing set.\n","\n","testing_accuracy_values = []\n","\n","print('Testing on slovenian data without additional training...')\n","\n","start_time = time.time()\n","\n","# Put the model in evaluation mode - the dropout layers behave differently during evaluation.\n","model.eval()\n","\n","# Tracking variables\n","test_accuracy = 0\n","num_of_batches = 0\n","\n","print('Number of testing comments: {:,}'.format(len(test_inputs)))\n","# Evaluate data for one epoch.\n","for batch in test_dataloader:\n","\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","\n","  # Unpack the inputs from out dataloader\n","  batch_input_ids, batch_attention_mask, batch_labels = batch\n","\n","  # Telling the model not to compute or store gradients, saves memory and speeds up validation\n","  with torch.no_grad():\n","      # Forward pass\n","      outputs = model(batch_input_ids, token_type_ids=None,\n","                      attention_mask=batch_attention_mask)\n","  \n","  # Get the \"logits\" output by the model, \"logits\" are the output values prior to applying an activation function\n","  logits = outputs[0]\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = batch_labels.to('cpu').numpy()\n","\n","  # Calculate the accuracy for this batch of test sentences\n","  acc = flat_accuracy(logits, label_ids)\n","\n","  # Accumulate the total accuracy\n","  test_accuracy += acc\n","\n","  # Track the number of batches\n","  num_of_batches += 1\n","\n","# Report the final accuracy for this testing run\n","accuracy = test_accuracy / num_of_batches\n","testing_accuracy_values.append(accuracy)\n","print('Accuracy: {0:.3f}'.format(accuracy))\n","print('Testing took: {:}'.format(format_time(time.time() - start_time)))\n","print('Testing done...')"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Testing on slovenian data without additional training...\n","Number of testing comments: 446\n","Accuracy: 0.470\n","Testing took: 0:00:16\n","Testing done...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_4QCz2teySE7","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}