{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lgbt_croatian_to_english_labbert.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1esoRePK3Rq7mlfgvnCsU0vMpwqmL6ppn","authorship_tag":"ABX9TyM1yznYbLhMiumd+fpGu14x"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"InHh4oiS4evq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":742},"outputId":"db0ccc73-688c-4e98-b2a9-4bc1ee96ada4","executionInfo":{"status":"ok","timestamp":1587574036289,"user_tz":-120,"elapsed":10012,"user":{"displayName":"Žan Pečovnik","photoUrl":"","userId":"03601017400239052827"}}},"source":["pip install transformers pytorch-pretrained-bert"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n","\r\u001b[K     |▋                               | 10kB 25.1MB/s eta 0:00:01\r\u001b[K     |█▏                              | 20kB 29.9MB/s eta 0:00:01\r\u001b[K     |█▊                              | 30kB 33.8MB/s eta 0:00:01\r\u001b[K     |██▎                             | 40kB 15.9MB/s eta 0:00:01\r\u001b[K     |███                             | 51kB 12.7MB/s eta 0:00:01\r\u001b[K     |███▌                            | 61kB 12.8MB/s eta 0:00:01\r\u001b[K     |████                            | 71kB 13.2MB/s eta 0:00:01\r\u001b[K     |████▋                           | 81kB 12.8MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 92kB 11.9MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 102kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 112kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 122kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 133kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 143kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 153kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 163kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 174kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 184kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 194kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 204kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 215kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 225kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 235kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 245kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 256kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 266kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 276kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 286kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 296kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 307kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 317kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 327kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 337kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 348kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 358kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 368kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 378kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 389kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 399kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 409kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 419kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 430kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 440kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 450kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 460kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 471kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 481kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 491kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 501kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 512kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 522kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 532kB 11.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 542kB 11.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 552kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 563kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 573kB 11.8MB/s \n","\u001b[?25hCollecting pytorch-pretrained-bert\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n","\r\u001b[K     |██▋                             | 10kB 31.3MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 14.6MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 19.8MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 11.7MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 13.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 15.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 14.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 14.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 14.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 14.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 14.3MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/50/93509f906a40bffd7d175f97fd75ea328ad9bd91f48f59c4bd084c94a25e/sacremoses-0.0.41.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 28.7MB/s \n","\u001b[?25hCollecting tokenizers==0.5.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n","\u001b[K     |████████████████████████████████| 3.7MB 54.1MB/s \n","\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n","\u001b[K     |████████████████████████████████| 1.0MB 46.6MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.40)\n","Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.4.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n","Requirement already satisfied: botocore<1.16.0,>=1.15.40 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.40)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.40->boto3->transformers) (0.15.2)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.40->boto3->transformers) (2.8.1)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.41-cp36-none-any.whl size=893334 sha256=4cba498c5017e90e1f4b7a5e682742835e2254fc18d94389aef212b851762809\n","  Stored in directory: /root/.cache/pip/wheels/22/5a/d4/b020a81249de7dc63758a34222feaa668dbe8ebfe9170cc9b1\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers, pytorch-pretrained-bert\n","Successfully installed pytorch-pretrained-bert-0.6.2 sacremoses-0.0.41 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.8.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WVCR1FvTEEgY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"outputId":"20be0651-140f-460a-b167-1dba9d302eba","executionInfo":{"status":"ok","timestamp":1587574040335,"user_tz":-120,"elapsed":13910,"user":{"displayName":"Žan Pečovnik","photoUrl":"","userId":"03601017400239052827"}}},"source":["import torch \n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":2,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla P4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UsLzaFY6EEdA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"outputId":"1f52703f-b89f-4a8b-c45c-52e83f4830ab","executionInfo":{"status":"ok","timestamp":1587574077423,"user_tz":-120,"elapsed":50837,"user":{"displayName":"Žan Pečovnik","photoUrl":"","userId":"03601017400239052827"}}},"source":["from transformers import BertForSequenceClassification, AdamW\n","from pytorch_pretrained_bert import BertTokenizer\n","\n","# Reload the saved model and vocabulary\n","print('Reloading the saved model and vocabulary...')\n","\n","TRAINED_LGBT_BERT = './drive/My Drive/Colab Notebooks/Bert models/LabBert_lgbt_croatian/'\n","\n","model = BertForSequenceClassification.from_pretrained(TRAINED_LGBT_BERT)\n","tokenizer = BertTokenizer.from_pretrained(TRAINED_LGBT_BERT, do_lower_case=False)\n","\n","# Tell the model to run on the GPU\n","model.cuda()\n","\n","optimizer = AdamW(model.parameters(), \n","                lr=2e-5, # learning rate, default = 5e-5\n","                eps=1e-8 # adam_epsilon, default = 1e-8\n","                )\n","\n","print('Done...')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Reloading the saved model and vocabulary...\n","Done...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"76r7Ss1hEEZa","colab_type":"code","colab":{}},"source":["import numpy as np\n","import time\n","import datetime\n","\n","# Helper function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","  pred_flat = np.argmax(preds, axis=1).flatten()\n","  labels_flat = labels.flatten()\n","  return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","# Helper function for formatting elapsed times\n","def format_time(elapsed):\n","  '''\n","  Takes a time in seconds and returns a string in format hh:mm:ss\n","  '''\n","  elapsed_rounded = int(round(elapsed))\n","  return str(datetime.timedelta(seconds=elapsed_rounded))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fyISJu7SEEWC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":84},"outputId":"1b0b7c58-6e0c-42bd-924a-a92ef9eb20dc","executionInfo":{"status":"ok","timestamp":1587574083759,"user_tz":-120,"elapsed":56855,"user":{"displayName":"Žan Pečovnik","photoUrl":"","userId":"03601017400239052827"}}},"source":["import pandas as pd\n","import gc\n","import math\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from torch.optim.lr_scheduler import StepLR\n","\n","MAX_LENGTH = 499\n","BATCH_SIZE = 8\n","\n","# Load the english data\n","print('Loading the english data...')\n","\n","CSV_FILE_PATH = './drive/My Drive/Colab Notebooks/diploma_data/lgbt_homophobia_final.csv'\n","df_slo = pd.read_csv(CSV_FILE_PATH, sep=',', header=None, names=['comment', 'label'])\n","\n","comments = df_slo.comment.values\n","labels = df_slo.label.values\n","\n","# Tokenize all of the comments and map the tokens to their word IDs\n","input_ids=[]\n","input_labels=[]\n","\n","for i,comment in enumerate(comments):\n","\n","  if isinstance(comment, float):\n","    continue\n","\n","  tokenized_comment = tokenizer.tokenize(comment)\n","  tokenized_comment.insert(0, '[CLS]')\n","  tokenized_comment.append('[SEP]')\n","\n","  if len(tokenized_comment) > 512:\n","    continue\n","  \n","  comment_ids = tokenizer.convert_tokens_to_ids(tokenized_comment)\n","  input_ids.append(comment_ids)\n","  input_labels.append(labels[i])\n","\n","# Delete the comments and labels so I free memory\n","del comments\n","del labels\n","gc.collect()\n","\n","print('Size of english dataset: {:,}'.format(len(input_ids)))\n","\n","print('Padding/truncating all the sentences to %d values...' % MAX_LENGTH)\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LENGTH, dtype='long', value=0, truncating='post', padding='post')\n","\n","attention_masks= []\n","for cmnt in input_ids:\n","  att_mask = [int(token_id > 0) for token_id in cmnt]\n","  attention_masks.append(att_mask)\n","\n","# size of training set is 80%, size of validation set is 20%\n","# random_state makes sure that the splitting is always the same\n","# split the validation set to actual validation set and testing set which is 10% of original size\n","train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, input_labels, random_state=420, test_size=0.2) \n","validation_inputs, test_inputs, validation_labels, test_labels = train_test_split(validation_inputs, validation_labels, random_state=420, test_size=0.5) # 0.5 * 0.2 = 0.1 -> 10% size of testing set\n","\n","train_masks, validation_masks, _, validation_masks_labels = train_test_split(attention_masks, input_labels, random_state=420, test_size=0.2)\n","validation_masks, test_masks, _, _ = train_test_split(validation_masks, validation_masks_labels, random_state=420, test_size=0.5)\n","\n","train_data = TensorDataset(torch.tensor(train_inputs), torch.tensor(train_masks), torch.tensor(train_labels))\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n","\n","validation_data = TensorDataset(torch.tensor(validation_inputs), torch.tensor(validation_masks), torch.tensor(validation_labels))\n","validation_sampler = SequentialSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=BATCH_SIZE)\n","\n","test_data = TensorDataset(torch.tensor(test_inputs), torch.tensor(test_masks), torch.tensor(test_labels))\n","test_sampler = SequentialSampler(test_data)\n","test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)\n","\n","# Create the learning rate scheduler\n","scheduler = StepLR(optimizer, step_size=5, gamma=0.1)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Loading the english data...\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["Size of english dataset: 5,895\n","Padding/truncating all the sentences to 499 values...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"w-516m_hEESj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":101},"outputId":"289e7c08-41b0-467f-8928-2636c5a8acfb","executionInfo":{"status":"ok","timestamp":1587574105577,"user_tz":-120,"elapsed":78464,"user":{"displayName":"Žan Pečovnik","photoUrl":"","userId":"03601017400239052827"}}},"source":["###################\n","#     Testing     #\n","###################\n","# Measure the performance on the english testing set.\n","\n","testing_accuracy_values = []\n","\n","print('Testing on english data without additional training...')\n","\n","start_time = time.time()\n","\n","# Put the model in evaluation mode - the dropout layers behave differently during evaluation.\n","model.eval()\n","\n","# Tracking variables\n","test_accuracy = 0\n","num_of_batches = 0\n","\n","print('Number of testing comments: {:,}'.format(len(test_inputs)))\n","# Evaluate data for one epoch.\n","for batch in test_dataloader:\n","\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","\n","  # Unpack the inputs from out dataloader\n","  batch_input_ids, batch_attention_mask, batch_labels = batch\n","\n","  # Telling the model not to compute or store gradients, saves memory and speeds up validation\n","  with torch.no_grad():\n","      # Forward pass\n","      outputs = model(batch_input_ids, token_type_ids=None,\n","                      attention_mask=batch_attention_mask)\n","  \n","  # Get the \"logits\" output by the model, \"logits\" are the output values prior to applying an activation function\n","  logits = outputs[0]\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = batch_labels.to('cpu').numpy()\n","\n","  # Calculate the accuracy for this batch of test sentences\n","  acc = flat_accuracy(logits, label_ids)\n","\n","  # Accumulate the total accuracy\n","  test_accuracy += acc\n","\n","  # Track the number of batches\n","  num_of_batches += 1\n","\n","# Report the final accuracy for this testing run\n","accuracy = test_accuracy / num_of_batches\n","testing_accuracy_values.append(accuracy)\n","print('Accuracy: {0:.3f}'.format(accuracy))\n","print('Testing took: {:}'.format(format_time(time.time() - start_time)))\n","print('Testing done...')"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Testing on english data without additional training...\n","Number of testing comments: 590\n","Accuracy: 0.724\n","Testing took: 0:00:22\n","Testing done...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"z5q1OvLNyTMa","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}