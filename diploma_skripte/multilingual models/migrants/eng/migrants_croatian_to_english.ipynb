{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"migrants_croatian_to_english.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1NLZZk9GLX9R8svA0jeOEnw3RN4ByCjl6","authorship_tag":"ABX9TyP2oGyaSJWLM0cj1DaJaTxP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"eGBxpaw845yb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":742},"outputId":"661c70ee-e62e-42d9-f6f7-c41c7f807fa7","executionInfo":{"status":"ok","timestamp":1587498945454,"user_tz":-120,"elapsed":10387,"user":{"displayName":"Žan Pečovnik","photoUrl":"","userId":"03601017400239052827"}}},"source":["pip install transformers pytorch-pretrained-bert"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n","\r\u001b[K     |▋                               | 10kB 27.2MB/s eta 0:00:01\r\u001b[K     |█▏                              | 20kB 32.4MB/s eta 0:00:01\r\u001b[K     |█▊                              | 30kB 20.9MB/s eta 0:00:01\r\u001b[K     |██▎                             | 40kB 13.6MB/s eta 0:00:01\r\u001b[K     |███                             | 51kB 12.0MB/s eta 0:00:01\r\u001b[K     |███▌                            | 61kB 11.1MB/s eta 0:00:01\r\u001b[K     |████                            | 71kB 10.7MB/s eta 0:00:01\r\u001b[K     |████▋                           | 81kB 10.5MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 92kB 10.4MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 102kB 10.5MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 112kB 10.5MB/s eta 0:00:01\r\u001b[K     |███████                         | 122kB 10.5MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 133kB 10.5MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 143kB 10.5MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 153kB 10.5MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 163kB 10.5MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 174kB 10.5MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 184kB 10.5MB/s eta 0:00:01\r\u001b[K     |███████████                     | 194kB 10.5MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 204kB 10.5MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 215kB 10.5MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 225kB 10.5MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 235kB 10.5MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 245kB 10.5MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 256kB 10.5MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 266kB 10.5MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 276kB 10.5MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 286kB 10.5MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 296kB 10.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 307kB 10.5MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 317kB 10.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 327kB 10.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 337kB 10.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 348kB 10.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 358kB 10.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 368kB 10.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 378kB 10.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 389kB 10.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 399kB 10.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 409kB 10.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 419kB 10.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 430kB 10.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 440kB 10.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 450kB 10.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 460kB 10.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 471kB 10.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 481kB 10.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 491kB 10.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 501kB 10.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 512kB 10.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 522kB 10.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 532kB 10.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 542kB 10.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 552kB 10.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 563kB 10.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 573kB 10.5MB/s \n","\u001b[?25hCollecting pytorch-pretrained-bert\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n","\r\u001b[K     |██▋                             | 10kB 33.1MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 15.1MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 20.2MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 12.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 11.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 13.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 13.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 13.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 13.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 13.3MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.40)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/50/93509f906a40bffd7d175f97fd75ea328ad9bd91f48f59c4bd084c94a25e/sacremoses-0.0.41.tar.gz (883kB)\n","\r\u001b[K     |▍                               | 10kB 28.6MB/s eta 0:00:01\r\u001b[K     |▊                               | 20kB 27.8MB/s eta 0:00:01\r\u001b[K     |█▏                              | 30kB 33.1MB/s eta 0:00:01\r\u001b[K     |█▌                              | 40kB 30.8MB/s eta 0:00:01\r\u001b[K     |█▉                              | 51kB 26.4MB/s eta 0:00:01\r\u001b[K     |██▎                             | 61kB 29.3MB/s eta 0:00:01\r\u001b[K     |██▋                             | 71kB 24.7MB/s eta 0:00:01\r\u001b[K     |███                             | 81kB 26.8MB/s eta 0:00:01\r\u001b[K     |███▍                            | 92kB 24.4MB/s eta 0:00:01\r\u001b[K     |███▊                            | 102kB 26.0MB/s eta 0:00:01\r\u001b[K     |████                            | 112kB 26.0MB/s eta 0:00:01\r\u001b[K     |████▌                           | 122kB 26.0MB/s eta 0:00:01\r\u001b[K     |████▉                           | 133kB 26.0MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 143kB 26.0MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 153kB 26.0MB/s eta 0:00:01\r\u001b[K     |██████                          | 163kB 26.0MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 174kB 26.0MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 184kB 26.0MB/s eta 0:00:01\r\u001b[K     |███████                         | 194kB 26.0MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 204kB 26.0MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 215kB 26.0MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 225kB 26.0MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 235kB 26.0MB/s eta 0:00:01\r\u001b[K     |█████████                       | 245kB 26.0MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 256kB 26.0MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 266kB 26.0MB/s eta 0:00:01\r\u001b[K     |██████████                      | 276kB 26.0MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 286kB 26.0MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 296kB 26.0MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 307kB 26.0MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 317kB 26.0MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 327kB 26.0MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 337kB 26.0MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 348kB 26.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 358kB 26.0MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 368kB 26.0MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 378kB 26.0MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 389kB 26.0MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 399kB 26.0MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 409kB 26.0MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 419kB 26.0MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 430kB 26.0MB/s eta 0:00:01\r\u001b[K     |████████████████                | 440kB 26.0MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 450kB 26.0MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 460kB 26.0MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 471kB 26.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 481kB 26.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 491kB 26.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 501kB 26.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 512kB 26.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 522kB 26.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 532kB 26.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 542kB 26.0MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 552kB 26.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 563kB 26.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 573kB 26.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 583kB 26.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 593kB 26.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 604kB 26.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 614kB 26.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 624kB 26.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 634kB 26.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 645kB 26.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 655kB 26.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 665kB 26.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 675kB 26.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 686kB 26.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 696kB 26.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 706kB 26.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 716kB 26.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 727kB 26.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 737kB 26.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 747kB 26.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 757kB 26.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 768kB 26.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 778kB 26.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 788kB 26.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 798kB 26.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 808kB 26.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 819kB 26.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 829kB 26.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 839kB 26.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 849kB 26.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 860kB 26.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 870kB 26.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 880kB 26.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 890kB 26.0MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n","Collecting tokenizers==0.5.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n","\u001b[K     |████████████████████████████████| 3.7MB 55.6MB/s \n","\u001b[?25hCollecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n","\u001b[K     |████████████████████████████████| 1.0MB 51.4MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.4.0)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n","Requirement already satisfied: botocore<1.16.0,>=1.15.40 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.40)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.40->boto3->transformers) (2.8.1)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.40->boto3->transformers) (0.15.2)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.41-cp36-none-any.whl size=893334 sha256=3a3de6b05c3849dda62b1a85a58bfe64a3585996dc964b7f11eeccc14c6eda64\n","  Stored in directory: /root/.cache/pip/wheels/22/5a/d4/b020a81249de7dc63758a34222feaa668dbe8ebfe9170cc9b1\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers, pytorch-pretrained-bert\n","Successfully installed pytorch-pretrained-bert-0.6.2 sacremoses-0.0.41 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.8.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kRpl1xPuE5XN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"outputId":"d4f40acd-9f49-409b-b69d-edd5a1b2e4b1","executionInfo":{"status":"ok","timestamp":1587498948853,"user_tz":-120,"elapsed":13623,"user":{"displayName":"Žan Pečovnik","photoUrl":"","userId":"03601017400239052827"}}},"source":["import torch \n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":2,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla P4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"g2LDl4dKE5T5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"outputId":"ac6b4e69-0c32-4dc7-fe2e-13cdb30ca319","executionInfo":{"status":"ok","timestamp":1587499138001,"user_tz":-120,"elapsed":56059,"user":{"displayName":"Žan Pečovnik","photoUrl":"","userId":"03601017400239052827"}}},"source":["from transformers import BertForSequenceClassification, AdamW\n","from pytorch_pretrained_bert import BertTokenizer\n","\n","# Reload the saved model and vocabulary\n","print('Reloading the saved model and vocabulary...')\n","\n","TRAINED_LGBT_BERT = './drive/My Drive/Colab Notebooks/Bert models/Bert_migrants_croatian/'\n","\n","model = BertForSequenceClassification.from_pretrained(TRAINED_LGBT_BERT)\n","tokenizer = BertTokenizer.from_pretrained(TRAINED_LGBT_BERT, do_lower_case=False)\n","\n","# Tell the model to run on the GPU\n","model.cuda()\n","\n","optimizer = AdamW(model.parameters(), \n","                lr=2e-5, # learning rate, default = 5e-5\n","                eps=1e-8 # adam_epsilon, default = 1e-8\n","                )\n","\n","print('Done...')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Reloading the saved model and vocabulary...\n","Done...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Mn2R-vuCE5RB","colab_type":"code","colab":{}},"source":["import numpy as np\n","import time\n","import datetime\n","\n","# Helper function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","  pred_flat = np.argmax(preds, axis=1).flatten()\n","  labels_flat = labels.flatten()\n","  return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","# Helper function for formatting elapsed times\n","def format_time(elapsed):\n","  '''\n","  Takes a time in seconds and returns a string in format hh:mm:ss\n","  '''\n","  elapsed_rounded = int(round(elapsed))\n","  return str(datetime.timedelta(seconds=elapsed_rounded))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rrkoy5adE5Op","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":84},"outputId":"dfbd7b91-4b9c-46c3-a5e1-8a3926e9daff","executionInfo":{"status":"ok","timestamp":1587499144266,"user_tz":-120,"elapsed":6287,"user":{"displayName":"Žan Pečovnik","photoUrl":"","userId":"03601017400239052827"}}},"source":["import pandas as pd\n","import gc\n","import math\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from torch.optim.lr_scheduler import StepLR\n","\n","MAX_LENGTH = 499\n","BATCH_SIZE = 8\n","\n","# Load the english data\n","print('Loading the english data...')\n","\n","CSV_FILE_PATH = './drive/My Drive/Colab Notebooks/diploma_data/migrants_islamophobia_final.csv'\n","df_slo = pd.read_csv(CSV_FILE_PATH, sep=',', header=None, names=['comment', 'label'])\n","\n","comments = df_slo.comment.values\n","labels = df_slo.label.values\n","\n","# Tokenize all of the comments and map the tokens to their word IDs\n","input_ids=[]\n","input_labels=[]\n","\n","for i,comment in enumerate(comments):\n","\n","  if isinstance(comment, float):\n","    continue\n","\n","  tokenized_comment = tokenizer.tokenize(comment)\n","  tokenized_comment.insert(0, '[CLS]')\n","  tokenized_comment.append('[SEP]')\n","\n","  if len(tokenized_comment) > 512:\n","    continue\n","  \n","  comment_ids = tokenizer.convert_tokens_to_ids(tokenized_comment)\n","  input_ids.append(comment_ids)\n","  input_labels.append(labels[i])\n","\n","# Delete the comments and labels so I free memory\n","del comments\n","del labels\n","gc.collect()\n","\n","print('Size of english dataset: {:,}'.format(len(input_ids)))\n","\n","print('Padding/truncating all the sentences to %d values...' % MAX_LENGTH)\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LENGTH, dtype='long', value=0, truncating='post', padding='post')\n","\n","attention_masks= []\n","for cmnt in input_ids:\n","  att_mask = [int(token_id > 0) for token_id in cmnt]\n","  attention_masks.append(att_mask)\n","\n","# size of training set is 80%, size of validation set is 20%\n","# random_state makes sure that the splitting is always the same\n","# split the validation set to actual validation set and testing set which is 10% of original size\n","train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, input_labels, random_state=420, test_size=0.2) \n","validation_inputs, test_inputs, validation_labels, test_labels = train_test_split(validation_inputs, validation_labels, random_state=420, test_size=0.5) # 0.5 * 0.2 = 0.1 -> 10% size of testing set\n","\n","train_masks, validation_masks, _, validation_masks_labels = train_test_split(attention_masks, input_labels, random_state=420, test_size=0.2)\n","validation_masks, test_masks, _, _ = train_test_split(validation_masks, validation_masks_labels, random_state=420, test_size=0.5)\n","\n","train_data = TensorDataset(torch.tensor(train_inputs), torch.tensor(train_masks), torch.tensor(train_labels))\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n","\n","validation_data = TensorDataset(torch.tensor(validation_inputs), torch.tensor(validation_masks), torch.tensor(validation_labels))\n","validation_sampler = SequentialSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=BATCH_SIZE)\n","\n","test_data = TensorDataset(torch.tensor(test_inputs), torch.tensor(test_masks), torch.tensor(test_labels))\n","test_sampler = SequentialSampler(test_data)\n","test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)\n","\n","# Create the learning rate scheduler\n","scheduler = StepLR(optimizer, step_size=5, gamma=0.1)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["Loading the english data...\n","Size of english dataset: 5,825\n","Padding/truncating all the sentences to 499 values...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1twd-PuJE5MC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":101},"outputId":"22ce8717-b6a1-4403-adc7-19d8e69a86fa","executionInfo":{"status":"ok","timestamp":1587499166887,"user_tz":-120,"elapsed":22645,"user":{"displayName":"Žan Pečovnik","photoUrl":"","userId":"03601017400239052827"}}},"source":["###################\n","#     Testing     #\n","###################\n","# Measure the performance on the english testing set.\n","\n","testing_accuracy_values = []\n","\n","print('Testing on english data without additional training...')\n","\n","start_time = time.time()\n","\n","# Put the model in evaluation mode - the dropout layers behave differently during evaluation.\n","model.eval()\n","\n","# Tracking variables\n","test_accuracy = 0\n","num_of_batches = 0\n","\n","print('Number of testing comments: {:,}'.format(len(test_inputs)))\n","# Evaluate data for one epoch.\n","for batch in test_dataloader:\n","\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","\n","  # Unpack the inputs from out dataloader\n","  batch_input_ids, batch_attention_mask, batch_labels = batch\n","\n","  # Telling the model not to compute or store gradients, saves memory and speeds up validation\n","  with torch.no_grad():\n","      # Forward pass\n","      outputs = model(batch_input_ids, token_type_ids=None,\n","                      attention_mask=batch_attention_mask)\n","  \n","  # Get the \"logits\" output by the model, \"logits\" are the output values prior to applying an activation function\n","  logits = outputs[0]\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = batch_labels.to('cpu').numpy()\n","\n","  # Calculate the accuracy for this batch of test sentences\n","  acc = flat_accuracy(logits, label_ids)\n","\n","  # Accumulate the total accuracy\n","  test_accuracy += acc\n","\n","  # Track the number of batches\n","  num_of_batches += 1\n","\n","# Report the final accuracy for this testing run\n","accuracy = test_accuracy / num_of_batches\n","testing_accuracy_values.append(accuracy)\n","print('Accuracy: {0:.3f}'.format(accuracy))\n","print('Testing took: {:}'.format(format_time(time.time() - start_time)))\n","print('Testing done...')"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Testing on english data without additional training...\n","Number of testing comments: 583\n","Accuracy: 0.588\n","Testing took: 0:00:22\n","Testing done...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oLhjEUTGT2Yh","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}